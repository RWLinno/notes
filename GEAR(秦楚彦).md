
Explainability in graph neural networks: A taxonomic survey是一篇关于图神经网络（Graph Neural Networks，GNNs）可解释性的论文。可解释性指的是能够解释和理解这些模型所做决策或预测的能力。随着GNN在社交网络分析、推荐系统和药物发现等领域的广泛应用，可解释性变得至关重要，以建立对其应用的信任和透明度。

这篇论文中的分类调查将图神经网络的可解释性方法按照其底层方法学进行了分类。以下是这个分类中的主要类别的概述：

1. **节点级解释**：这些方法侧重于为图中的单个节点提供解释。它们旨在确定对特定节点的预测有贡献的重要特征或邻居。可以使用显著性图、注意力机制和影响函数等技术来突出显示相关特征或邻居。
2. **图级解释**：这些方法旨在解释图神经网络在图级别上做出的整体预测。它们通常关注识别对输出有显著贡献的重要子图、模式或全局结构。可以利用图摘要、子图提取和影响最大化等技术进行图级解释。
3. **基于规则的解释**：基于规则的方法旨在从GNN中提取人类可理解的规则或模式。这些规则可以帮助解释模型如何做出决策。可以使用符号规则提取、决策树或关联规则挖掘等技术生成可解释的规则。
4. **局部解释**：局部解释侧重于解释模型在特定预测周围的行为。它们旨在确定对该特定预测有贡献的最具影响力的节点、边或子图。可以使用LIME（局部可解释模型无关解释）和SHAP（SHapley Additive exPlanations）等技术来提供局部解释。
5. **全局解释**：与局部解释相反，全局解释旨在理解模型在整个图上的行为。它们关注影响整体预测的全局属性、拓扑结构或社区模式。图注意力、图卷积滤波器和图自注意机制等技术可以提供对模型决策过程的见解。
6. **混合解释**：混合方法结合多种解释技术，以更全面地理解GNNs。它们利用不同方法的优势同时捕捉节点级

