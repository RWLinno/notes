---
title : 05-数据解析
date : 2021-6-20
tags: 网络爬虫
---



# 数据解析



#### 编码流程

-指定Url
-发起请求
-获取响应数据
-数据解析
-持久化存储



#### 分类

-正则
-bs4
-xpath(重点)



#### 原理

-解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储。
-进行指定标签的定位
-标签或者标签对应的属性中存储的数据值进行提取（解析）

样例

```python
#需求：爬取图片数据
import requests
if __name__=='__main__':
    url='https://pic.qiushibaike.com/system/pictures/12405/124053508/medium/0Y1VC3QIIZDN8W32.jpg'
    #content返回的是二进制形式的图片数据
    #text(字符串) content(二进制) json(对象)
    img_data=requests.get(url=url).content
    with open('./qiutu.jpg','wb') as fp:
        fp.write(img_data)
```



#### 实战代码

```python
#需求：正则表达式爬取板块所有图片
import requests
import re
import os
if __name__ =='__main__':
    #创建文件夹保存所有图片
    if not os.path.exists('./qiutuLibs'):
        os.mkdir('./qiutuLibs')
    url = 'https://www.qiushibaike.com/imgrank/page/%d/'
    headers={
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3861.400 QQBrowser/10.7.4313.400'
    }
    for pageNum in range(1,10):
        #对应页码的Url
        new_url=format(url%pageNum)
        #使用通用爬虫对url对应的一整张页面进行爬取
        page_text=requests.get(url=new_url,headers=headers).text
        #使用聚焦爬虫将页面中所有的糗图进行解析和提取
        ex='<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
        img_src_list=re.findall(ex,page_text,re.S)
        print(img_src_list)
        for src in img_src_list:
            #拼接出一个完整的图片url
            src='https:'+src
            #请求到了图片的二进制数据
            img_data=requests.get(url=src,headers=headers).content
            #生成图片名称
            img_name=src.split('/')[-1]
            #图片最终存储的路径
            imgPath='./qiutuLibs/'+img_name
            with open(imgPath,'wb',) as fp:
                fp.write(img_data)
                print(img_name,'下载成功！！！')
```



#### ba4详解

##### ba4进行数据解析原理

1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取



##### 环境安装

-pip install bs4
-pip instal lxml



##### 如何实例化BeautifulSoup对象

-from bs4 import BeautifulSoup
-对象实例化的两种形式：
  1.将本地的Html文档中的数据加载到该对象中
    fp=open('./test.html','r',encoding='utf-8')
    soup=BeautifulSoup(fp,'lxml')
  2.将互联网上获取的页面源码加载到该对象中
    page_text=response.text
    soup=BeatufulSoup(page_text,'html')
-提供的用于数据解析的方法和属性：

  - soup.tagName返回的是文档中第一次出现的tagName对应的标签
  - soup.find()：
      -find('tagName')：等同于soup.div
      -属性定位：
          -soup.find('div',class_/id/attr='song')
    -soup.find_all('tagName'):返回符合要求的所有标签（列表）

-select:
	-select('某种选择器(id,class,标签...选择器)')，返回的是一个列表。
	-层级选择器：
		-soup.select('.tang > ul > li > a'):大于号表示一个层级
		-soup.select('.tang > ul a'):空格表示的是多个层级
-获取标签之间的文本数据：
	-soup.a.text/string/get_text()
	-text/get_text()：可以获取某一个标签中所有的文本内容
	-string：只可以获取该标签下面直系的文本内容
-获取标签中属性值：
	-soup.a['href']



#### Xpath详解

##### xpath解析

最常用且最便捷高效的一种解析方式。通用性强。

##### xpath解析原理

1.实例化一个etree的对象，且需将被解析的页面源码数据加载到该对象中。
2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的获取。

##### 环境的安装

​	-pip install lxml

##### 如何实例化一个etree对象

from lxml import etree
1.将本地的html文档中的源码数据加载到etree对象中：
etree.parse(filePath)
2.可以将从互联网上获取的源码数据加载到该对象中
etree.HTML('page_text')
3.xpath('xpath表达式')：
/：表示的是从根节点开始定位。表示一个层级
//：表示的是多个层级。可以表示从任意位置开始定位
属性定位：//div[@class='song'] tag[@attrName='attrValue']
索引定位：//div[@class='song']/p[3] 索引是从1开始的
取文本：
/text():获取的是标签中直系的文本内容
//text():标签中非直系的文本内容
取属性：
/@attrName  ==>img/src



#### 实战代码

```python
#需求：爬取58同城二手房中的房源信息
import  requests
from lxml import etree

if __name__=='__main__':
    #爬取到页面源码数据
    url='https://bj.58.com/ershoufang/'
    headers={
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'
    }
    page_text=requests.get(url=url,headers=headers).text
    #数据解析
    tree=etree.HTML(page_text)
    li_list=tree.xpath('//url[@class="house-list-wrap"]/li')
    for li in li_list:
        title=li.xpath('./li/div[2]/h2/a/text()')[0]
        print(title)
        fp.write(title+'\n')
```



```python
#需求：爬取全国热门城市及所有城市
import requests
from lxml import etree
if __name__=='__main__':
    headers={
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3861.400 QQBrowser/10.7.4313.400'
    }
    url='https://www.aqistudy.cn/historydata/'
    page_text=requests.get(url=url,headers=headers).text
    tree=etree.HTML(page_text)
    #解析到热门城市和所有城市对应的a标签
    #div/ul/li/a  热门城市a标签的层级关系
    #div/ul/div[2]/li/a  全部城市a标签的层级关系
    a_list=tree.xpath('//div[@class="bottom"]/ul/li/a | //div[@class="bottom"]/ul/div[2]/li/a ')
    all_city_names = []
    for a in a_list:
        city_name=a.xpath('./text()')[0]
        all_city_names.append(city_name)
    print(all_city_names,len(all_city_names))
```

