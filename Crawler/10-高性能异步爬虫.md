---
title : 10-高性能异步爬虫
date : 2021-6-20
tags: 网络爬虫
---



# 高性能异步爬虫

####目的

在爬虫中使用异步实现高性能的数据爬取操作。



#### 异步爬虫的方式

	-1.多线程，多进程（不建议）：
		好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。
		弊端：无法无限制的开启多线程或者多进程
	-2.线程池、进程池（适当的使用）：
		好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。
		弊端：池中线程或进程的数量是有上限的。
		原则：线程池处理的是阻塞且耗时的操作。
	 -3.单线程+异步协程（推荐）:
	 	event_loop:事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行
	 	coroutine:协议对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用async关键字来定义一个方法，这个方法在调用时不会立刻被执行，而是返回一个协程对象。
		task:任务，它是对协程对象的进一步封装，包含了任务的各种状态。
		future:代表将来执行或还没有执行的任务，实际上和task没有本质区别。
		async:定义一个协程
		await:用来挂起阻塞方法的执行。





####实战代码

```python
#需求：爬取梨视频的视频数据
import requests
from lxml import etree
import re
import os
from multiprocessing.pool import Pool
if __name__=='__main__':
    url='https://www.pearvideo.com/category_5'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'
    }
    page_text=requests.get(url=url,headers=headers).text
    tree=etree.HTML(page_text)
    li_list=tree.xpath('//*[@id="listvideoListUl"]/li')
    urls=[] #存储所有视频的连接
    for li in li_list:
        detail_url='https://www.pearvideo.com/'+li.xpath('./div/a/@href')[0]
        name=li.xpath('./div/a/div[2]/text()')[0]+'.mp4'
        print(detail_url,name)
        #对详情页的url发请求
        detail_page_text=requests.get(url=detail_url,headers=headers).text
        #从详情页中解析出视频的地址（url）
        #  ex='staUrl="(.*?)",vdoUrl'
        #  video_url=re.findall(ex,detail_page_text)[0]  这个方法已经不能用了
        tree=etree.HTML(detail_page_text)
        video_url=tree.xpath('//*[@id="JprismPlayer"]')
        print(video_url,'视频地址')
        dic={
            'name':name,
            'url':video_url,
        }
        urls.append(dic)

    def get_video_data(dic):
        url=dic['url']
        print(dic['name'],'正在下载......')
        data=requests.get(url=url,headers=headers).content
        #持久化存储操作
        with open(dic['name'],'wb') as fp:
            fp.write(data)
            print(dic['name'],'下载成功')

    #使用线程池对视频数据进行请求（较为耗时的阻塞操作）
    pool = Pool(2)
    #pool.map(get_video_data,urls)
    pool.close()
    pool.join()
  
```



####协程介绍

```python

import asyncio
async def request(url):
    print('正在请求的url是',url)
    print('请求成功',url)
    return url
    #async修饰的函数，调用之后返回的是一个协程对象
c=request('www..baidu.com')
    #创建一个事件循环对象
    #loop=asyncio.get_event_loop()
    #将协程对象注册到loop中，然后启动loop
    #loop.run_until_complete(c)

#task的使用
#loop=asyncio.get_event_loop()
#基于loop创建了一个task对象
#task=loop.create_task(c)
#print(task)
#loop.run_until_complete(task)
#print(task)

#future的使用
#loop=asyncio.get_event_loop()
#task=asyncio.ensure_future()
#print(task)
#loop.run_until_complete(task)
#print(task)

def callback_func(task):
    #result返回的就是任务对象中封装的协程对象对应函数的返回值
    print(task.result()):
#绑定回调
loop=asyncio.get_event_loop()
task=asyncio.ensure_future()
#将回调函数绑定到任务对象中
task.add_done_callback(callback_func())
loop.run_until_complete(task)
```



#### 多任务异步协程

```python
import asyncio
import time

async def request(url):
    print('正在下载',url)
    #在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步
    await asyncio.sleep(2)
    print('下载完毕',url)

start=time.time()

urls=[
    'www.baidu.com',
    'www.sogou.com',
    'www.goubanjia.com'
]

#任务列表：存放多个任务对象
tasks=[]
for url in urls:
    c=request(url)
    task=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
#需要将任务列表封装到wait中
loop.run_until_complete(asyncio.wait(tasks))
print(time.time()-start)
```





#### Aiohttp简单应用

```python
import requests
import asyncio
import time
start=time.time()
urls=[
    'http://127.0.0.1:5000/bobo',
    'http://127.0.0.1:5000/jay',
    'http://127.0.0.1:5000/rwl'
]
async def get_page(url):
    print('正在下载',url)
    #requests.get是基于同步的，必须使用基于异步的网络请求模块进行指定url的请求发送
    #aiohttp:基于异步网络请求的模块
    response=requests.get(url=url)
    print('下载完毕：',response.text)

tasks=[]

for url in urls:
    c=get_page(url)
    tasks=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
emd=time.time()
print('总耗时',end-start)
```



#### Aiohttp实现多任务异步协程

环境安装：pip install aiohttp

使用该模块中的ClientSession

```python
import requests
import asyncio
import time
import aiohttp

start=time.time()
urls=[
    'http://127.0.0.1:5000/bobo',
    'http://127.0.0.1:5000/jay',
    'http://127.0.0.1:5000/rwl'
]
async def get_page(url):
    async with aiohttp.ClientSession() as session:
        #get()、post():
        #headers，params/data,proxy='http://ip:port'
        async with await session.get(url) as response:
            #text()返回字符串形式的响应数据
            #read()返回的是二进制形式的响应数据
            #json()返回的就是json对象
            #注意：获取响应数据操作之前一定要使用await进行手动挂起
            page_text=response.text()
            print(page_text)

tasks=[]

for url in urls:
    c=get_page(url)
    task=asyncio.ensure_future(c)
    tasks.append(task)

loop=asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
emd=time.time()
print('总耗时',end-start)
```

